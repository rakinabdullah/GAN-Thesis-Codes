{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import numpy as np\n",
    "import cv2\n",
    "from glob import glob\n",
    "from matplotlib import pyplot\n",
    "from sklearn.utils import shuffle\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Resizing the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Created directory: C:\\Users\\User\\Downloads\\archive\\OCT2017/train_resized\n",
      "* Created directory: C:\\Users\\User\\Downloads\\archive\\OCT2017/train_resized/CNV\n",
      "***** Completed Resizing Images *****\n",
      "* Created directory: C:\\Users\\User\\Downloads\\archive\\OCT2017/train_resized/DME\n",
      "***** Completed Resizing Images *****\n",
      "* Created directory: C:\\Users\\User\\Downloads\\archive\\OCT2017/train_resized/DRUSEN\n",
      "***** Completed Resizing Images *****\n",
      "* Created directory: C:\\Users\\User\\Downloads\\archive\\OCT2017/train_resized/NORMAL\n",
      "***** Completed Resizing Images *****\n"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "main_path = r\"C:\\Users\\User\\Downloads\\archive\\OCT2017\" + \"/train\"\n",
    "# main_path = \"C:/Users/18201142/Downloads/OCT2017\" + \"/test\"\n",
    "\n",
    "\n",
    "def directory(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(f\"* Created directory: {path}\")\n",
    "\n",
    "\n",
    "def resize_images(img_path: str, out_path: str):\n",
    "    directory(out_path)\n",
    "    for img in os.listdir(img_path):\n",
    "        img_file = f\"{img_path}/{img}\"\n",
    "        if os.path.isfile(img_file):\n",
    "            im = Image.open(img_file)\n",
    "            imResize = im.resize((28,28),Image.ANTIALIAS)\n",
    "            imResize.save(f\"{out_path}/{img}\", \"JPEG\", quality=100)\n",
    "    print(f\"***** Completed Resizing Images *****\")\n",
    "\n",
    "\n",
    "for label in os.listdir(main_path):\n",
    "    output_path = f\"{main_path+'_resized'}\"\n",
    "    directory(f\"{output_path}\")\n",
    "    resize_images(f\"{main_path}/{label}\", f\"{output_path}/{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Converted to C:\\Users\\User\\Downloads\\archive\\OCT2017\\train_resized/CNV `.npz`\n",
      "* Converted to C:\\Users\\User\\Downloads\\archive\\OCT2017\\train_resized/DME `.npz`\n",
      "* Converted to C:\\Users\\User\\Downloads\\archive\\OCT2017\\train_resized/DRUSEN `.npz`\n",
      "* Converted to C:\\Users\\User\\Downloads\\archive\\OCT2017\\train_resized/NORMAL `.npz`\n",
      "Dataset converted to npz and saved here at D:\\dataset\\train_to_npz_compressed \n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>image</th>\n",
       "      <th>label</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>CNV/CNV-1016042-1.npz</td>\n",
       "      <td>CNV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>CNV/CNV-1016042-10.npz</td>\n",
       "      <td>CNV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>CNV/CNV-1016042-100.npz</td>\n",
       "      <td>CNV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CNV/CNV-1016042-101.npz</td>\n",
       "      <td>CNV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>CNV/CNV-1016042-102.npz</td>\n",
       "      <td>CNV</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     image label\n",
       "0    CNV/CNV-1016042-1.npz   CNV\n",
       "1   CNV/CNV-1016042-10.npz   CNV\n",
       "2  CNV/CNV-1016042-100.npz   CNV\n",
       "3  CNV/CNV-1016042-101.npz   CNV\n",
       "4  CNV/CNV-1016042-102.npz   CNV"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import pandas as pd\n",
    "\n",
    "COMPRESSED = True\n",
    "image_dataset_dir = r\"C:\\Users\\User\\Downloads\\archive\\OCT2017\\train_resized\"\n",
    "new_dataset_folder = r\"D:\\dataset\\train_to_npz\" + \"_compressed\" if COMPRESSED else \"\"\n",
    "\n",
    "dataset = {\n",
    "    \"image\" :[],\n",
    "    \"label\" : []\n",
    "}\n",
    "\n",
    "for label in os.listdir(image_dataset_dir):\n",
    "    images_dir = f\"{image_dataset_dir}/{label}\"\n",
    "    if not os.path.isdir(images_dir):\n",
    "        continue\n",
    "    for image_file in os.listdir(images_dir):\n",
    "        if not image_file.endswith(\".jpeg\"):\n",
    "            continue\n",
    "        img = load_img(os.path.join(image_dataset_dir, label, image_file))\n",
    "        x = img_to_array(img)\n",
    "\n",
    "        rel_path = label + \"/\" + os.path.splitext(image_file)[0] + '.npz'\n",
    "        os.makedirs(new_dataset_folder + \"/\" + label, exist_ok=True)\n",
    "        npz_file = os.path.join(new_dataset_folder, rel_path)\n",
    "        # True: Compressed, False: UnCompressed\n",
    "        if COMPRESSED:\n",
    "            np.savez_compressed(npz_file, x)\n",
    "        else:\n",
    "            np.savez(npz_file, x)\n",
    "        dataset[\"image\"].append(rel_path)\n",
    "        dataset[\"label\"].append(label)\n",
    "    print(f\"* Converted to {images_dir} `.npz`\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(os.path.join(new_dataset_folder, \"train.csv\"), index=False)\n",
    "\n",
    "print('Dataset converted to npz and saved here at %s '%new_dataset_folder)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] The system cannot find the path specified: 'D:/18201142/dataset/OCT2017/test'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Input \u001b[1;32mIn [5]\u001b[0m, in \u001b[0;36m<cell line: 23>\u001b[1;34m()\u001b[0m\n\u001b[0;32m     19\u001b[0m             imResize\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mout_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mimg\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mJPEG\u001b[39m\u001b[38;5;124m\"\u001b[39m, quality\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m)\n\u001b[0;32m     20\u001b[0m     \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m***** Completed Resizing Images *****\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m---> 23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m label \u001b[38;5;129;01min\u001b[39;00m \u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlistdir\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmain_path\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[0;32m     24\u001b[0m     output_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmain_path\u001b[38;5;241m+\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_resized\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m     25\u001b[0m     directory(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00moutput_path\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] The system cannot find the path specified: 'D:/18201142/dataset/OCT2017/test'"
     ]
    }
   ],
   "source": [
    "from PIL import Image\n",
    "import os, sys\n",
    "\n",
    "main_path = \"D:/18201142/dataset/OCT2017\" + \"/test\"\n",
    "\n",
    "def directory(path: str):\n",
    "    if not os.path.exists(path):\n",
    "        os.mkdir(path)\n",
    "        print(f\"* Created directory: {path}\")\n",
    "\n",
    "\n",
    "def resize_images(img_path: str, out_path: str):\n",
    "    directory(out_path)\n",
    "    for img in os.listdir(img_path):\n",
    "        img_file = f\"{img_path}/{img}\"\n",
    "        if os.path.isfile(img_file):\n",
    "            im = Image.open(img_file)\n",
    "            imResize = im.resize((256,256),Image.ANTIALIAS)\n",
    "            imResize.save(f\"{out_path}/{img}\", \"JPEG\", quality=100)\n",
    "    print(f\"***** Completed Resizing Images *****\")\n",
    "\n",
    "\n",
    "for label in os.listdir(main_path):\n",
    "    output_path = f\"{main_path+'_resized'}\"\n",
    "    directory(f\"{output_path}\")\n",
    "    resize_images(f\"{main_path}/{label}\", f\"{output_path}/{label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "from tensorflow.keras.preprocessing.image import img_to_array, load_img\n",
    "import pandas as pd\n",
    "\n",
    "COMPRESSED = True\n",
    "image_dataset_dir = r\"D:/18201142/dataset/OCT2017/test_resized\"\n",
    "new_dataset_folder = r\"C:/Users/18201142/Downloads/OCT2017/test_to_npz\" + \"_compressed\" if COMPRESSED else \"\"\n",
    "\n",
    "dataset = {\n",
    "    \"image\" :[],\n",
    "    \"label\" : []\n",
    "}\n",
    "\n",
    "for label in os.listdir(image_dataset_dir):\n",
    "    images_dir = f\"{image_dataset_dir}/{label}\"\n",
    "    if not os.path.isdir(images_dir):\n",
    "        continue\n",
    "    for image_file in os.listdir(images_dir):\n",
    "        if not image_file.endswith(\".jpeg\"):\n",
    "            continue\n",
    "        img = load_img(os.path.join(image_dataset_dir, label, image_file))\n",
    "        x = img_to_array(img)\n",
    "\n",
    "        rel_path = label + \"/\" + os.path.splitext(image_file)[0] + '.npz'\n",
    "        os.makedirs(new_dataset_folder + \"/\" + label, exist_ok=True)\n",
    "        npz_file = os.path.join(new_dataset_folder, rel_path)\n",
    "        # True: Compressed, False: UnCompressed\n",
    "        if COMPRESSED:\n",
    "            np.savez_compressed(npz_file, feature=x)\n",
    "        else:\n",
    "            np.savez(npz_file, x)\n",
    "        dataset[\"image\"].append(rel_path)\n",
    "        dataset[\"label\"].append(label)\n",
    "    print(f\"* Converted to {images_dir} `.npz`\")\n",
    "\n",
    "\n",
    "df = pd.DataFrame(dataset)\n",
    "df.to_csv(os.path.join(new_dataset_folder, \"test.csv\"), index=False)\n",
    "\n",
    "print('Dataset converted to npz and saved here at %s '%new_dataset_folder)\n",
    "\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset into training and testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Visiting : `train_resized`\n",
      "* Visiting : `train_resized/CNV`\n",
      "* Visiting : `train_resized/DME`\n",
      "* Visiting : `train_resized/DRUSEN`\n",
      "* Visiting : `train_resized/NORMAL`\n",
      "X_train: (83484, 28, 28)\n",
      "y_train: (83484,)\n",
      "X_test: (0,)\n",
      "y_test: (0,)\n",
      "* Created `thesis-dataset.npz` in `D:\\dataset`\n"
     ]
    }
   ],
   "source": [
    "from os import listdir, path\n",
    "\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "# root_dir = \"/content/drive/MyDrive/Colab Notebook Files/OCT2017\"\n",
    "root_dir = r\"C:\\Users\\User\\Downloads\\archive\\OCT2017\"\n",
    "out_dir = r\"D:\\dataset\"\n",
    "\n",
    "label_to_int = {\n",
    "    \"CNV\": 0,\n",
    "    \"DME\": 1,\n",
    "    \"DRUSEN\": 2,\n",
    "    \"NORMAL\": 3,\n",
    "}\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "for target in listdir(root_dir):\n",
    "    # target: \"test\" or \"train\" directory\n",
    "    if not \"resized\" in target:\n",
    "        continue\n",
    "    dir_lvl_1 = f\"{root_dir}/{target}\"\n",
    "    print(f\"* Visiting : `{target}`\")\n",
    "    if path.isfile(dir_lvl_1):\n",
    "        continue\n",
    "    for label in listdir(dir_lvl_1):\n",
    "        # label: \"CNV\" or \"DME\" or \"DRUSEN\" or \"NORMAL\"\n",
    "        dir_lvl_2 = f\"{dir_lvl_1}/{label}\"\n",
    "        print(f\"* Visiting : `{target}/{label}`\")\n",
    "        for file in listdir(f\"{dir_lvl_2}\"):\n",
    "            # print(f\"* Reading : `{target}/{label}/{file}`\")\n",
    "            img = cv2.imread(f\"{dir_lvl_2}/{file}\", cv2.IMREAD_GRAYSCALE)\n",
    "            if \"test\" in target:\n",
    "                X_test.append(img)\n",
    "                y_test.append(label_to_int[label])\n",
    "            elif \"train\" in target:\n",
    "                X_train.append(img)\n",
    "                y_train.append(label_to_int[label])\n",
    "\n",
    "X_train = np.array(X_train, dtype=np.uint8)\n",
    "y_train = np.array(y_train, dtype=np.uint8)\n",
    "X_test = np.array(X_test, dtype=np.uint8)\n",
    "y_test = np.array(y_test, dtype=np.uint8)\n",
    "\n",
    "print(\"X_train:\", X_train.shape)\n",
    "print(\"y_train:\", y_train.shape)\n",
    "print(\"X_test:\", X_test.shape)\n",
    "print(\"y_test:\", y_test.shape)\n",
    "\n",
    "np.savez_compressed(out_dir + \"/thesis-dataset.npz\", X_train=X_train, y_train=y_train, X_test=X_test, y_test=y_test)\n",
    "print(f\"* Created `thesis-dataset.npz` in `{out_dir}`\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_train (83484, 28, 28)\n",
      "y_train (83484,)\n",
      "X_test (0,)\n",
      "y_test (0,)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# root_dir = \"/content/drive/MyDrive/Colab Notebook Files/OCT2017\"\n",
    "root_dir = r\"C:\\Users\\User\\Downloads\\archive\\OCT2017\"\n",
    "out_dir = r\"D:\\dataset\"\n",
    "\n",
    "data = np.load(out_dir + \"/thesis-dataset.npz\")\n",
    "for k, v in data.items():\n",
    "    print(k, data[k].shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_train = data.get(\"y_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83484,)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = data.get(\"X_train\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83484, 28, 28)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import\n",
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "from tensorflow.keras.layers import Input\n",
    "from tensorflow.keras.optimizers import RMSprop\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.keras.models import load_model\n",
    "\n",
    "import numpy as np\n",
    "import argparse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "id": "DNPEnn_mtdLM"
   },
   "outputs": [],
   "source": [
    "IMG_SHAPE = (28, 28, 1)\n",
    "BATCH_SIZE = 512\n",
    "\n",
    "# Size of the noise vector\n",
    "noise_dim = 128\n",
    "\n",
    "\n",
    "# Reshape each sample to (28, 28, 1) and normalize the pixel values in the [-1, 1] range\n",
    "train_images = X_train.reshape(X_train.shape[0], *IMG_SHAPE).astype(\"float32\")\n",
    "train_images = (train_images - 127.5) / 127.5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(83484, 28, 28, 1)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_images.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the discriminator (the critic in the original WGAN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"discriminator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 28, 28, 1)]       0         \n",
      "                                                                 \n",
      " zero_padding2d (ZeroPadding  (None, 32, 32, 1)        0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d (Conv2D)             (None, 16, 16, 64)        1664      \n",
      "                                                                 \n",
      " leaky_re_lu (LeakyReLU)     (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " conv2d_1 (Conv2D)           (None, 8, 8, 128)         204928    \n",
      "                                                                 \n",
      " leaky_re_lu_1 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " conv2d_2 (Conv2D)           (None, 4, 4, 256)         819456    \n",
      "                                                                 \n",
      " leaky_re_lu_2 (LeakyReLU)   (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " dropout_1 (Dropout)         (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " conv2d_3 (Conv2D)           (None, 2, 2, 512)         3277312   \n",
      "                                                                 \n",
      " leaky_re_lu_3 (LeakyReLU)   (None, 2, 2, 512)         0         \n",
      "                                                                 \n",
      " flatten (Flatten)           (None, 2048)              0         \n",
      "                                                                 \n",
      " dropout_2 (Dropout)         (None, 2048)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 2049      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 4,305,409\n",
      "Trainable params: 4,305,409\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def conv_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    padding=\"same\",\n",
    "    use_bias=True,\n",
    "    use_bn=False,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.5,\n",
    "):\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "    x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_discriminator_model():\n",
    "    img_input = layers.Input(shape=IMG_SHAPE)\n",
    "    # Zero pad the input to make the input images size to (32, 32, 1).\n",
    "    x = layers.ZeroPadding2D((2, 2))(img_input)\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        64,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        use_bias=True,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        128,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        256,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=True,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "    x = conv_block(\n",
    "        x,\n",
    "        512,\n",
    "        kernel_size=(5, 5),\n",
    "        strides=(2, 2),\n",
    "        use_bn=False,\n",
    "        activation=layers.LeakyReLU(0.2),\n",
    "        use_bias=True,\n",
    "        use_dropout=False,\n",
    "        drop_value=0.3,\n",
    "    )\n",
    "\n",
    "    x = layers.Flatten()(x)\n",
    "    x = layers.Dropout(0.2)(x)\n",
    "    x = layers.Dense(1)(x)\n",
    "\n",
    "    d_model = keras.models.Model(img_input, x, name=\"discriminator\")\n",
    "    return d_model\n",
    "\n",
    "\n",
    "d_model = get_discriminator_model()\n",
    "d_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Generator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"generator\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_2 (InputLayer)        [(None, 128)]             0         \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 4096)              524288    \n",
      "                                                                 \n",
      " batch_normalization (BatchN  (None, 4096)             16384     \n",
      " ormalization)                                                   \n",
      "                                                                 \n",
      " leaky_re_lu_4 (LeakyReLU)   (None, 4096)              0         \n",
      "                                                                 \n",
      " reshape (Reshape)           (None, 4, 4, 256)         0         \n",
      "                                                                 \n",
      " up_sampling2d (UpSampling2D  (None, 8, 8, 256)        0         \n",
      " )                                                               \n",
      "                                                                 \n",
      " conv2d_4 (Conv2D)           (None, 8, 8, 128)         294912    \n",
      "                                                                 \n",
      " batch_normalization_1 (Batc  (None, 8, 8, 128)        512       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_5 (LeakyReLU)   (None, 8, 8, 128)         0         \n",
      "                                                                 \n",
      " up_sampling2d_1 (UpSampling  (None, 16, 16, 128)      0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_5 (Conv2D)           (None, 16, 16, 64)        73728     \n",
      "                                                                 \n",
      " batch_normalization_2 (Batc  (None, 16, 16, 64)       256       \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " leaky_re_lu_6 (LeakyReLU)   (None, 16, 16, 64)        0         \n",
      "                                                                 \n",
      " up_sampling2d_2 (UpSampling  (None, 32, 32, 64)       0         \n",
      " 2D)                                                             \n",
      "                                                                 \n",
      " conv2d_6 (Conv2D)           (None, 32, 32, 1)         576       \n",
      "                                                                 \n",
      " batch_normalization_3 (Batc  (None, 32, 32, 1)        4         \n",
      " hNormalization)                                                 \n",
      "                                                                 \n",
      " activation (Activation)     (None, 32, 32, 1)         0         \n",
      "                                                                 \n",
      " cropping2d (Cropping2D)     (None, 28, 28, 1)         0         \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 910,660\n",
      "Trainable params: 902,082\n",
      "Non-trainable params: 8,578\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "def upsample_block(\n",
    "    x,\n",
    "    filters,\n",
    "    activation,\n",
    "    kernel_size=(3, 3),\n",
    "    strides=(1, 1),\n",
    "    up_size=(2, 2),\n",
    "    padding=\"same\",\n",
    "    use_bn=False,\n",
    "    use_bias=True,\n",
    "    use_dropout=False,\n",
    "    drop_value=0.3,\n",
    "):\n",
    "    x = layers.UpSampling2D(up_size)(x)\n",
    "    x = layers.Conv2D(\n",
    "        filters, kernel_size, strides=strides, padding=padding, use_bias=use_bias\n",
    "    )(x)\n",
    "\n",
    "    if use_bn:\n",
    "        x = layers.BatchNormalization()(x)\n",
    "\n",
    "    if activation:\n",
    "        x = activation(x)\n",
    "    if use_dropout:\n",
    "        x = layers.Dropout(drop_value)(x)\n",
    "    return x\n",
    "\n",
    "\n",
    "def get_generator_model():\n",
    "    noise = layers.Input(shape=(noise_dim,))\n",
    "    x = layers.Dense(4 * 4 * 256, use_bias=False)(noise)\n",
    "    x = layers.BatchNormalization()(x)\n",
    "    x = layers.LeakyReLU(0.2)(x)\n",
    "\n",
    "    x = layers.Reshape((4, 4, 256))(x)\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        128,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x,\n",
    "        64,\n",
    "        layers.LeakyReLU(0.2),\n",
    "        strides=(1, 1),\n",
    "        use_bias=False,\n",
    "        use_bn=True,\n",
    "        padding=\"same\",\n",
    "        use_dropout=False,\n",
    "    )\n",
    "    x = upsample_block(\n",
    "        x, 1, layers.Activation(\"tanh\"), strides=(1, 1), use_bias=False, use_bn=True\n",
    "    )\n",
    "    # At this point, we have an output which has the same shape as the input, (32, 32, 1).\n",
    "    # We will use a Cropping2D layer to make it (28, 28, 1).\n",
    "    x = layers.Cropping2D((2, 2))(x)\n",
    "\n",
    "    g_model = keras.models.Model(noise, x, name=\"generator\")\n",
    "    return g_model\n",
    "\n",
    "\n",
    "g_model = get_generator_model()\n",
    "g_model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create the WGAN-GP model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "class WGAN(keras.Model):\n",
    "    def __init__(\n",
    "        self,\n",
    "        discriminator,\n",
    "        generator,\n",
    "        latent_dim,\n",
    "        discriminator_extra_steps=3,\n",
    "        gp_weight=10.0,\n",
    "    ):\n",
    "        super(WGAN, self).__init__()\n",
    "        self.discriminator = discriminator\n",
    "        self.generator = generator\n",
    "        self.latent_dim = latent_dim\n",
    "        self.d_steps = discriminator_extra_steps\n",
    "        self.gp_weight = gp_weight\n",
    "\n",
    "    def compile(self, d_optimizer, g_optimizer, d_loss_fn, g_loss_fn):\n",
    "        super(WGAN, self).compile()\n",
    "        self.d_optimizer = d_optimizer\n",
    "        self.g_optimizer = g_optimizer\n",
    "        self.d_loss_fn = d_loss_fn\n",
    "        self.g_loss_fn = g_loss_fn\n",
    "\n",
    "    def gradient_penalty(self, batch_size, real_images, fake_images):\n",
    "        \"\"\"Calculates the gradient penalty.\n",
    "\n",
    "        This loss is calculated on an interpolated image\n",
    "        and added to the discriminator loss.\n",
    "        \"\"\"\n",
    "        # Get the interpolated image\n",
    "        alpha = tf.random.normal([batch_size, 1, 1, 1], 0.0, 1.0)\n",
    "        diff = fake_images - real_images\n",
    "        interpolated = real_images + alpha * diff\n",
    "\n",
    "        with tf.GradientTape() as gp_tape:\n",
    "            gp_tape.watch(interpolated)\n",
    "            # 1. Get the discriminator output for this interpolated image.\n",
    "            pred = self.discriminator(interpolated, training=True)\n",
    "\n",
    "        # 2. Calculate the gradients w.r.t to this interpolated image.\n",
    "        grads = gp_tape.gradient(pred, [interpolated])[0]\n",
    "        # 3. Calculate the norm of the gradients.\n",
    "        norm = tf.sqrt(tf.reduce_sum(tf.square(grads), axis=[1, 2, 3]))\n",
    "        gp = tf.reduce_mean((norm - 1.0) ** 2)\n",
    "        return gp\n",
    "\n",
    "    def train_step(self, real_images):\n",
    "        if isinstance(real_images, tuple):\n",
    "            real_images = real_images[0]\n",
    "\n",
    "        # Get the batch size\n",
    "        batch_size = tf.shape(real_images)[0]\n",
    "\n",
    "        # For each batch, we are going to perform the\n",
    "        # following steps as laid out in the original paper:\n",
    "        # 1. Train the generator and get the generator loss\n",
    "        # 2. Train the discriminator and get the discriminator loss\n",
    "        # 3. Calculate the gradient penalty\n",
    "        # 4. Multiply this gradient penalty with a constant weight factor\n",
    "        # 5. Add the gradient penalty to the discriminator loss\n",
    "        # 6. Return the generator and discriminator losses as a loss dictionary\n",
    "\n",
    "        # Train the discriminator first. The original paper recommends training\n",
    "        # the discriminator for `x` more steps (typically 5) as compared to\n",
    "        # one step of the generator. Here we will train it for 3 extra steps\n",
    "        # as compared to 5 to reduce the training time.\n",
    "        for i in range(self.d_steps):\n",
    "            # Get the latent vector\n",
    "            random_latent_vectors = tf.random.normal(\n",
    "                shape=(batch_size, self.latent_dim)\n",
    "            )\n",
    "            with tf.GradientTape() as tape:\n",
    "                # Generate fake images from the latent vector\n",
    "                fake_images = self.generator(random_latent_vectors, training=True)\n",
    "                # Get the logits for the fake images\n",
    "                fake_logits = self.discriminator(fake_images, training=True)\n",
    "                # Get the logits for the real images\n",
    "                real_logits = self.discriminator(real_images, training=True)\n",
    "\n",
    "                # Calculate the discriminator loss using the fake and real image logits\n",
    "                d_cost = self.d_loss_fn(real_img=real_logits, fake_img=fake_logits)\n",
    "                # Calculate the gradient penalty\n",
    "                gp = self.gradient_penalty(batch_size, real_images, fake_images)\n",
    "                # Add the gradient penalty to the original discriminator loss\n",
    "                d_loss = d_cost + gp * self.gp_weight\n",
    "\n",
    "            # Get the gradients w.r.t the discriminator loss\n",
    "            d_gradient = tape.gradient(d_loss, self.discriminator.trainable_variables)\n",
    "            # Update the weights of the discriminator using the discriminator optimizer\n",
    "            self.d_optimizer.apply_gradients(\n",
    "                zip(d_gradient, self.discriminator.trainable_variables)\n",
    "            )\n",
    "\n",
    "        # Train the generator\n",
    "        # Get the latent vector\n",
    "        random_latent_vectors = tf.random.normal(shape=(batch_size, self.latent_dim))\n",
    "        with tf.GradientTape() as tape:\n",
    "            # Generate fake images using the generator\n",
    "            generated_images = self.generator(random_latent_vectors, training=True)\n",
    "            # Get the discriminator logits for fake images\n",
    "            gen_img_logits = self.discriminator(generated_images, training=True)\n",
    "            # Calculate the generator loss\n",
    "            g_loss = self.g_loss_fn(gen_img_logits)\n",
    "\n",
    "        # Get the gradients w.r.t the generator loss\n",
    "        gen_gradient = tape.gradient(g_loss, self.generator.trainable_variables)\n",
    "        # Update the weights of the generator using the generator optimizer\n",
    "        self.g_optimizer.apply_gradients(\n",
    "            zip(gen_gradient, self.generator.trainable_variables)\n",
    "        )\n",
    "        return {\"d_loss\": d_loss, \"g_loss\": g_loss}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create a Keras callback that periodically saves generated images"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GANMonitor(keras.callbacks.Callback):\n",
    "    def __init__(self, num_img=6, latent_dim=128):\n",
    "        self.num_img = num_img\n",
    "        self.latent_dim = latent_dim\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs=None):\n",
    "        random_latent_vectors = tf.random.normal(shape=(self.num_img, self.latent_dim))\n",
    "        generated_images = self.model.generator(random_latent_vectors)\n",
    "        generated_images = (generated_images * 127.5) + 127.5\n",
    "\n",
    "        for i in range(self.num_img):\n",
    "            img = generated_images[i].numpy()\n",
    "            img = keras.preprocessing.image.array_to_img(img)\n",
    "            img.save(\"generated_img_{i}_{epoch}.png\".format(i=i, epoch=epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/20\n",
      "164/164 [==============================] - 41s 177ms/step - d_loss: -7.3305 - g_loss: -11.2052\n",
      "Epoch 2/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -5.3684 - g_loss: 2.8924\n",
      "Epoch 3/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -4.2625 - g_loss: 2.4194\n",
      "Epoch 4/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -3.4430 - g_loss: 2.9075\n",
      "Epoch 5/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -2.8039 - g_loss: 3.0411\n",
      "Epoch 6/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -2.2656 - g_loss: 2.7720\n",
      "Epoch 7/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -1.8468 - g_loss: 4.6222\n",
      "Epoch 8/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -1.6722 - g_loss: 4.0131\n",
      "Epoch 9/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -1.4984 - g_loss: 3.0093\n",
      "Epoch 10/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.4708 - g_loss: 1.5855\n",
      "Epoch 11/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.4210 - g_loss: 1.9351\n",
      "Epoch 12/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.3584 - g_loss: 0.3359\n",
      "Epoch 13/20\n",
      "164/164 [==============================] - 29s 176ms/step - d_loss: -1.3303 - g_loss: 0.2006\n",
      "Epoch 14/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.2821 - g_loss: 1.9858\n",
      "Epoch 15/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.2344 - g_loss: 1.3110\n",
      "Epoch 16/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.2282 - g_loss: 1.3672\n",
      "Epoch 17/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.1810 - g_loss: 1.4286\n",
      "Epoch 18/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.1405 - g_loss: 2.0342\n",
      "Epoch 19/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.1167 - g_loss: 1.8467\n",
      "Epoch 20/20\n",
      "164/164 [==============================] - 29s 175ms/step - d_loss: -1.0964 - g_loss: 2.2519\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1e23d888a90>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Instantiate the optimizer for both networks\n",
    "# (learning_rate=0.0002, beta_1=0.5 are recommended)\n",
    "generator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "discriminator_optimizer = keras.optimizers.Adam(\n",
    "    learning_rate=0.0002, beta_1=0.5, beta_2=0.9\n",
    ")\n",
    "\n",
    "# Define the loss functions for the discriminator,\n",
    "# which should be (fake_loss - real_loss).\n",
    "# We will add the gradient penalty later to this loss function.\n",
    "def discriminator_loss(real_img, fake_img):\n",
    "    real_loss = tf.reduce_mean(real_img)\n",
    "    fake_loss = tf.reduce_mean(fake_img)\n",
    "    return fake_loss - real_loss\n",
    "\n",
    "\n",
    "# Define the loss functions for the generator.\n",
    "def generator_loss(fake_img):\n",
    "    return -tf.reduce_mean(fake_img)\n",
    "\n",
    "\n",
    "# Set the number of epochs for trainining.\n",
    "epochs = 20\n",
    "\n",
    "# Instantiate the customer `GANMonitor` Keras callback.\n",
    "cbk = GANMonitor(num_img=3, latent_dim=noise_dim)\n",
    "\n",
    "# Get the wgan model\n",
    "wgan = WGAN(\n",
    "    discriminator=d_model,\n",
    "    generator=g_model,\n",
    "    latent_dim=noise_dim,\n",
    "    discriminator_extra_steps=3,\n",
    ")\n",
    "\n",
    "# Compile the wgan model\n",
    "wgan.compile(\n",
    "    d_optimizer=discriminator_optimizer,\n",
    "    g_optimizer=generator_optimizer,\n",
    "    g_loss_fn=generator_loss,\n",
    "    d_loss_fn=discriminator_loss,\n",
    ")\n",
    "\n",
    "# Start training\n",
    "wgan.fit(train_images, batch_size=BATCH_SIZE, epochs=epochs, callbacks=[cbk])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Display the generated image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAAB/0lEQVR4nD3M3W5TSRBF4b2rqo//sBMnmYRwMYqQRkhzBeL9Jd4CHmAkBiMxSWyf2Ke7a3OBmHW99PFeHUhDAgKZKVqzNJTmccE9m6xHo6yDRqSTYENEVzMV2Yx6AaynZPiltHg7Pj23emqeTsrEtBYAM3IV25t83B/HE+vUe0eEyz0SamDsTw/2Jo4bXHz+8thR5NUSVFTVqF93h3n0BW/87eFlaifhPBWgZvNY3B7m+6OP2vnylsAwDD9eNCm7LWP5+uriv11tdbcbn17N21hyfVnPp/LH3Q0/voLFljdrlNO/zHZ49rO11bD+89s//LCt56PHzOf3ZaynuMrLJ//h3Jevyb832RuGnpP3WemrYZuHOoz7tnhYxfS9axaZguF09szc5HEsd/fLLt65SPfujXJGaYthfXnt842+7WIqoGpIVb14V44Th/nzZIc488KEEEGYYBwAWxWctdaU0YpSdPT0NFIwO1pMqpaVc5eMFGRwgI6BZJOiK9KSSABEiiZJRjaypUKiMQlRktxSMLkarUdIUAogBEvAuhINnS0ZEDoFM6VAJZQGZbqkMIIgJZkgwBNNIJJSSAAgYyaZKQgQCQGKTNCADkG/RqcIUAji/wjzWMRmecVFIJ/9gfNOFnNtCq//erd9/3o1m5mTAsRP1zaszKYSUdxgvykK+AnhBklsYPDCGAAAAABJRU5ErkJggg==\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACTklEQVR4nCXK2Y4UVRgA4H87VdXTiwyGQZFBTSSaEL3RKx+HN+QZeABDYnTQGBwl6aZn6apeajnnX7jg+vvwxU40IsApAFwQTNDExiTN9/J45xwcaEChNcmUrKKBzx58Dgt59LciAkS4CqmLMR6BK45tDlkGqbiFIrNCI8B5wfOpXDT8ozQ5yEiCZhSuk+Moy/ppXX8p71m+ozzj4tBAxkzFq8uf+ls7Dl27+UHaYurkNhJP5Wz54rP173nmNx2cp6fyc7Ioiyw1Y/XF81/4zZ4WCetnj77ylVyqpEKBmvjBw4e3/7TPH9/2x/rJ5WnD+OHlxjRpGepmPjvuLs7H9m759er+RqtfRbECmHBezWy3XSzzuzJ/hnc3xZvVO9r/N547+JSxeD1PkcX3fTe2427NsrzY3LQph20LzyHjiJidQxOGStl20+HMTraotL2XsFOugLwMVP8v4hZgwEiZROhQEmRABlQRGQdExzmcMsaRJkdzCAhQ9lpOqkDu2JxB9NlEAioQz1JxK/0wJfVCFfWOdRoyUgY1pFIGSaKeqzR0jbNkMwY1rlkVoZEuypin0SHQDJzqTBCZwDl6uXxyyKdi6kTgWDw8DADR2B23r+TPq/Xhbhg8ObEnUOdwdBHB/o0fdn9d/dYdMxaHkIDAAA5GEvlGD2nsDuv2EFOhkgo6QCByuKDUFOerb9f/XrUAyUsJAAQgZ0E3CMMydtNmd/26vT72ZhAB6FihAwAABIS77vu3b9fbPz7cl0DD5hNifAoOWrLut++vrvdtWn0E2SSS4h0aawgAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACaklEQVR4nAXBu25cVRQA0P065z7mETsTE0MMJDYNQW5SUNAjOv4BiZJf4gP4BCQ6akCyZAriIhJJyMTjzNi+c889r71ZC58mZUIz9pgYutPPvzs77gUBAA2fqTjF7CrUaFx8doen559986QnBHxWiZUysroqbFX3Rpa/+OrbF6s5fpm0U1DGAok9+Kg+5txN7RG9wDNVhgpUjExcZYE+ivm8n3Ze/MCanIlSJgSwife4Z4Ue3bF02bxzlcGgVAJBAGemwTuMuCzSs1BLNGQhVJx6LGyVm/EB/vTrbamtxfSAmlqN2yJQGkwaQosXH/65+qvYdnKzATaRFLSDnLH1zTn+0M1WG53LfbwJ+MeGurZpxl6Xq1n7tbRv7m3mGzedHD+9gavNIzn4yD9+eLteb3/DX/rd+/8+bHHpH61OusuLbSxP+nLqSlKPP3ftEsb1n8Orw/H5vPkU19v8XmvqD9njj7ac9Y2bYH/9ryu8eLhdwZ3tQRvsBTfrHv2qaZYn59fp8l3It+no5JPZu6ns8PsBeu3GEWdnj6dZR3AXblbtYIr1Gp/XOZjH+9liXPDHzRx5sb7bhc5DMJFp8DxJcOMU0+iOJuNcA7/RthTZVo3O52xDkCiL22De8hCohgpSYyRmqEEqWMDkcB9iSU3BSlJEChWcmJUTj4mDTBVMcxNAKJsxV7OMJJYVUi5qhoWKSspsBTU584mSA+RUCilYTAcS98KuLTVTBCN0WkkrqRomk3nKJe4NgVs9yAqcs1UDUwbDl6//vnp5sSmUpHpQFEJQMCyQV6gAZmX3+u3vby/v46QGgKRaKtrB/4NIgpv2cJmRAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAABwAAAAcCAAAAABXZoBIAAACTklEQVR4nAXBW2/URhQA4HPOnBnb6012N0QbKKUCJIQQEpVQH8oTj/x/8QAiVSSaNo0Cuay9Xtsz59Lvw6eBlKriGD2NWOPgLNHEq2lUbistkSJBUBYXRctigd1iCIwijqmgqqg4GjAQIaHNaLxSk3qg4gW0JEByDgoK2Uz5w4vr814oKRDFWjRVMYROfCC74xeb4XgIQ5BDqjJggCnn5bIcEvDElJZn1u+nnNFHTQiAknOZhHDET2/6ypt/4aZLu/4AhLCW9aLMRCS8arlxgX3MEjYn1m6evJdNvL14uJwyP369OZe/7m7qfljC9uTtx9+ffdMvP6/7f2TA909O+xPv6NW+G//mJp7K6W59f7WzMYwsu/t6uWnT5mwxLL7SpfffW32+eDpdNh3PI9ePjuy225fxKuPrPzcamtXVrr8sM+euLlZV+BIWXXy3Wi0DhJ+fby/Ga6uYTLEf2mb5y69w4/zw3/dDhbVu6224xz8sjDge+/N0fMSPwuHHRQnezB3s1DmwGFjJ51NsqD1adD+yyiJP1UwBf6sLkdac9pEnQ545WIi5+JRnBkV3UjRwisEIE+5VVPxQnLlyN6ICJuhSUIupOypSKCyHMBsARSP3UgolUBB3m0zJRAkREF1noYjE4DUAh4TEEAnRDAzA3Nwgi5B4sSLEkzC4KFUJtTAJzua5uFIxxyPg6KKRG8/mMJIacAGD7C1XghYLTDaRZXAjMgRAjJWtOc2zV8XcjFADuoqzOYATGZ+1lreqi/UEz4ZtPUyH3TU+FCkYjv8HnAGSRPpsY08AAAAASUVORK5CYII=\n",
      "text/plain": [
       "<IPython.core.display.Image object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import Image, display\n",
    "\n",
    "display(Image(\"generated_img_0_19.png\"))\n",
    "display(Image(\"generated_img_1_19.png\"))\n",
    "display(Image(\"generated_img_2_19.png\"))\n",
    "display(Image(\"generated_img_2_3.png\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
